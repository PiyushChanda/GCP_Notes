# Storage Options
Q - Databases are abstractions on File Storage, File Storage are abstractions on __ storage?
A - File Storage is an abstraction over Block storage, in Block storage, we indentify a file by it's physical hardware address.
In File Storage, we indentify a file, by a clear heirarchical directory structure.

Block Storage for compute VMs - presistent Disks or SSDs
Immutable Blobs like Video/Media/Image - Cloud Storage
OLTP - Cloud SQL - Open Source MySQL/Postgre SQL or Cloud Spanner -Complicated and Google's Proprietery
NoSQL Documents - HTML/XML Called - Datastore
Columnar Data Storge - NoSQL key-value store similar HBase or BigTable
Getting data into cloud storage - Transfer Service

When You Need | What you use | GCP Counterpart
Storage for compute/block storage | Hard Disks/SSDs | Persistent Disk (Hard Disk, SSD), Also Local SSD
Storing Media/Blocks which are not very mutable | file system - HDFS | Cloud Storage, rather than hdfs - hdfs require running NameNode
SQL Interface Atop a data file | Hive (SQL Like - but mapreduce running on HDFS) | Semanitcally, BigQuery is similar to Hive, Underlying implementation is different. BigQuery is Columnar, Hive is a mapreduce abstraction
Documents, NoSQL | Extremely Hierarchical Datasets - HTML File/XML File | CouchDB, MongoDB - KeyValue/Indexed Databases, we have a bunch of keys for every document. Single Key - Reddis | Datatore, blazing fast read, scales with the size of the document, is based on indexes on every column
Columnar Store, NoSQL | HBase for fast scanning, Cassandra, rows stored in lexicographical order of particular key value. Optmised for looking up row-keys | Big Table is HBase, fast sequential scanning of key-values
large no. of sequential key-values
OLTP | RDBMs, can we improve the storage, taking into account the heirarchical documents. | cloud sql - open, cloud spanner - proprietory, optmizations under the hood, really strong asset gurantees
Analytics and Data Warehousing applications that run on HIVE. simply called OLAP applications. | Hive | Big Query - already referred to

** Mobile Specific **
Storage for Compute for Block storage and with mobile sdks | Cloud storage with firebase
Fast Random access with mobile sdk | Firebase reatime DB

Q - Match the following
I Hive a.Datastore
II HBase b.BigTable
III MongoDB c.BigQuery

A -
I Hive c. BigQuery
II HBase b. BigTable
III MongoDB a.DataStore

Block storage - Refers to data stored in cylinders of some other physical forms that is one level lower in granularity than File Storage
Files are decomposed into blocks. Unstructured data.
1. Lowest data storage
2. No Abstraction at all
3. Meant for VMs, for cpus doing low level io access
4. The block location must be tied to VMs

Options Available 
1. Persistent Disks - Standard or SSD - Speed for random access, for sequential access, however, the standard disks are not that bad
2. Local SSD - 3 TB - Attached to a particular VM

Cloud Storage - 
1. creates buckets to store data
2. Buckets are globally unique, globally unique, similar to DNS rules.
3. Also locations and storage classes

Bucket storage classes
1. Multi regional - for frequent access from all over the world
2. Regional - for frequent access from a particular region
3. Nearline - accessed once a month, have minimum storage duration commitments, and are subtantially cheaper than regional or multi-regional storage, can be very expensive for accessing your data
4. Coldline - accessed once a year, other things are similar to NearLine

In the real world, we have hdfs, in the gcp, we use cloud storage, as hdfs requires an always running namenode which would unnecessarily add to the cost, even when all we need is blob storage
Cloud storage doesnot require a central managing namenaode.
Even in hadoop cluster/data proc, we make use of the cloud storage rather than the hdfs.

SQL Interfaces for file data - BigQuery, we need to have an sql like interface over the top of a file, because we might require
a ton of analytics solutions on the top of files and the BI People know to write SQL. BigQuery is like Hive, semantically atleast.

Differences -
1. It is faster than hive, lower latency than hive, as it relies on a columnar data storage format.
We can atleast think about using hive for low latency real-time processing. We cannot even think of using hive for low-latency
real time processing
Latency is a bit higher than Big Table, Data Store - prefer them for low latency
2. No ACID Properties of Database, so never use it for OLTP, Atomicity, Consistency, Isolation, Durability, consists of an all or nothing unit
3. Great for OLAP/Business Intelligence/Data Warehouesing
4. OLTP Requires strict write consistency, OLAP Doesnot.
5. Use cases similar to hive, sql-like abstraction for Non-relational data, unlike hive, doesnot rely on hdfs

Transaction Processing/Transaction Support - 
Cloud SQL
Cloud Spanner
1. Structured data, constraints
2. ACID Properties - used for transaction processing. Both of them supprt ACID properties, but the cloud spanner supports something
of a ACID++ support.
3. Too slow or too many checks for analytics/BI Ware househing (OLAP)
4. OLTP require strict write consistency, OLAP doesnot

So if your data is really big or not really structured, then use big-query otherwise, small, super structured, use cloud-sql/spanner-sql

Spanner SQL is proprietory and more advanced than cloud sql
Cloud Spanner offeres "horizontal scaling" - i.e. more data, more instances, replication etc. Between bigdata and mysql-rdbms

Cloud Spanner under the hood has a pretty suprising structure. We will discuss it later.

OLAP and OLTP have sql on their core, let's move on to mysql database

Document Storage using Datastore -  Mongo DB or Couch DB
xml/html
Hierarchical tree like relationsships, as well as a great deal of latitude on what data is embedded in what nodes.
At heart, they are key-value structures.
key-value structure, Not a single key column

It is not used for OTLP or OLAP, it is used when we require a fast lookup on a bunch of keys.
Speciality of Datastore is that the speed of query execution depends on the size of the returned set, and not on the size of underlying dataset.
It is all about very fast hash based indices
So for needle in the heystack like searches or lookup for non-sequential keys, Indices are fast to read, slow to write
So don't use for write intensive data.

It offers fairly decent transaction support. as it needs to write and get things quite right.
Better transaction options than other storage options we have been discussing.

NoSQL - Columnar data storage BigTable - HBase
When we require scanning of sequential key-values.
Columnar Datastore - good for Sparse Data
Data is sorted on keyvalue and sorted in lexicographically similar values of the keys are stored adjacent to each other.
Sensetive to hotspoting. If we chose increasing numerical column as our key, then it would kill the performance of Big Table as
any read/write operation would hit the same shard of data. We need to be careful while designing the key-structure.
Bigtable is Similar to HBase, somehow smarter than HBase.


## Cloud Storage

Create Buckets for data storage - names need to be globally unique, location and storage classes.
Multi-regional - For frequently accessed data, that need to be accessed all over the world
Regional - For frequently accessed data, that needs to be accessed from a specific location
Nearline - For Hot data, we would wound up with a big bill
Coldline - For Hot data, we would wound up with a big bill

Multiregional - 
1 .99.95 % availability and Geo-redundant
2. For storing dynamic content like serving website content, streaming videos, or gaming and mobile applications
3. $0.0026 per GB per Month
4. Cloud storage stores the data in atleast two regions separated by atleast 100 miles within the multi-regional location of the bucket.
5. Highest availability

Regional Storage -
1. 99.9% Availaibility
2. Lower cost for GB
3. Data stored in narrow geographic location
4. Suitable for storing frequently accessed in the same region as your google cloud data proc, or google compute engine instances
that use it, such as for data analytics
5. $0.02 per GB per Month
6. Better performance in data-intensive applications, rather than storing your data in multi-regional location

Nearline Storage -
1. 99% Availability
2. Very low cost per GB per month, $0.01
3. Data retrieval costs
4. Higher-per operation cost
5. 30-day minimum storage duration
6. Data you want to access only once a month. for serving long-tail multi-media content, also good for backup, disaster recovery, and archival storage
7. If you want to access it frequently, you will have to pay access charges

Coldline Storage - 
1. It basically has the same throughput and latency, i.e. not slower in any way.
2. It is a 90-day minimum storage duration
3. costs for data access
4. Higher per-operation costs
5. 99% availiability SLA
6. For disaster recovery, and for data that may or may not be required in some future time
7. per-operations

How one works with cloud storage
1. XML and JSON APIs
2. command line gsutil
3. gcp console
4. client apis

Domain Named Buckets - Discussed during the use case of Hosting a website
gcp treats a bucket name as if they were a domain name, if they contain dots. So it must be a valid domain name.
Must be a syntactically valid domain name/DNS
For Eg - it should no contain a name that has more than one period in a row
for eg www....dummybucket.com
end with a currently recognised top level domain, i.e. .com
pass domain ownership verification. The team member creating the bucket must be a domain owner or a domain manager.

Domain Verification - 
There are a number of ways to demostrate the ownership of a site or a webpage
1. Adding special meta tag to the site's homepage
2. Uploading a special file to the site
3. viewing ownership for the domain through the search console

Mobile level use-cases
1. When you need storage for compute block storage, along with mobile sdk's. The mobile sdk's here are referred to the
the api's that help with uploading and downloading of content, mostly user generated. Firebase provides an additional level of security while uploading and downloading user generated content. - Cloud Storage with Firebase
For mobile developers.
2. Firebase Realtime DB. Fast No SQL json based document database that helps with fast random access along with mobile sdks.
Syncs data with all the clients in realtime, and is available even when your app goes offline.

Three Lines - Gcloud Menu - Browser, all the buckets reside here
Since they are globally unique, you can prefix your names with project prefix/organisation prefix.
Buckets are standalone, they are not tied to a compute instance, container or App Engine projects
The data we store in the bucket are called objects.
 - Simply use the create button to create a bucket, choose regional class and a region, and finally click 'upload' to upload the data to the bucket.
 - Every object has a storage class assossiated with it. An object can have a class assossiated with it.
 - The uploaded image can have 'near-line' or 'cold-line' class, but not 'multi-regional'
 - The objects by default take up the storage classes of the bucket. The storage class of a bucket can be changed, but the objects retain the storage classes that they have
 - If we are not using the resource, or the bucket, simply delete it to avoid unnecessary costs


Q - Let us suppose I want to make the data in the container public. How would we do that?

## Assigning permissions to buckets and objects

To share a particular object publicly, we can click on the share publicly radio button. Once the image has been made public,
we can click on the public link, and that should give us a sharable link to the object.
storage.googleapis.com/<name of the bucket>/<folder name>/<name of the object>

To edit the permissions of a particular object, we could click on the 3 dots and click on edit permissions
The permission can be given to a domain, a group or a individual user.
Change entity to 'user' or 'group' as per your needs.
We can also specify a domain name. Everyone from this domain will be able to read the document or own it completely.
So to change the bucket permission, go back to the bucket browser, and click on the 3 dots on the very right.
Click edit permissions, here you can simply specify a user or a domain, and assign roles to them, in the vid, the domain 
loonytunes.com was assigned as a storage admin by us - owner of the bucket.
We can assign similar roles to a user for the bucket, objects in the bucket take on their individual permissions.

Q - How do I automatically delete my log files after a month on cloud storage.
A - We can simply set up a specific lifecycle setting for the bucket.

gsutil mb -c regional -l asia-east1 gs://loony-asia-bucket

gsutil is a python object that lets you perform cloud storage task from the command line.
such as copying and retrieving stuff from buckets, and so on.
create a new bucket - mb make bucket

To list all the regions that google cloud offers, we can use 'gcloud compute regions list'

gsutil ls - list all the buckets
gsutil cp - source and destination bucket, can be used to copy files over the buckets
gsutil cp -r -p gs://loony-us-bucket/* gs://loony-asia-bucket/
-r -- recursively copies the files
-p -- permissions of the individual objects are copied over as well

Life Cycle Management - Allows you to specify how long the files are going to stick around before a specific event is triggered.

We can get any existing lifecycle setting set on the bucket by typing the following command
gsutil lifecycle get gs://loony-asia-bucket

We specify the lifecycle of a bucket in a json file.
{
  "rule":[
     {
     "action" : {"type":"Delete"},
     "condition" : {"age":"30"}
     }
  ]
}

so we type the above code in a json file and simply use a gsutil tool to set the lifecycle
gsutil lifecycle set loony_asia_lifecycle.json gs://loony-asia-bucket

we can verify the data lifecycle setting configured in the bucket by typing
gsutil lifecycle get gs://loony-asia-bucket


gsutil config -b - this command supposedly fixes some kind of AccessDeniedException: Insufficient permission.

Q - If data generated by the program running on compute engine is stored on a cloud bucket, then where would the data go if we delete the VM Instance

We will see how to ingest the data stored in the cloud storage, using a VM Engine instance


